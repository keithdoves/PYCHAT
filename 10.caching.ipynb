{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669f85c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import SQLiteCache\n",
    "\n",
    "\n",
    "# - 캐싱을 사용하면 LM(언어모델)의 응답을 저장할 수 있음\n",
    "# - 똑같은 질문을 계속 받는다면, 이미 답변한 답의 캐싱을 이용하여 재사용 할 수 있음\n",
    "# - `set_llm_cache(InMEmoryCache())` : `response`가 메모리에 저장됨\n",
    "# - `set_debug(True)`: 무슨일을 실행하는지에 대한 로그를 보여줌\n",
    "\n",
    "set_llm_cache(SQLiteCache(\"cache.db\"))\n",
    "#이렇게 하면 SQLite DB로 캐싱된다. cache.db파일이 생성됨\n",
    "#redis 등 다양한 캐싱방법이 있음(langchain 홈페이지 참조)\n",
    "#set_llm_cache(InMemoryCache())\n",
    "#이렇게 하면 모든 response가 메모리에 저장됨\n",
    "set_debug(True)\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-4\",\n",
    "temperature=0.1,\n",
    "streaming=True,\n",
    ")\n",
    "\n",
    "res = chat.invoke(\"How do you make italian pasta\")\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
